name: Performance Benchmarks

on:
  workflow_dispatch: # Manual trigger
  pull_request:
    paths:
      - 'TASVideos.Parsers/**'
      - 'TASVideos.ForumEngine/**'
      - 'TASVideos.WikiEngine/**'
      - 'TASVideos.Core/**'
      - 'TASVideos.Data/**'
      - 'TASVideos.Common/**'
      - 'tests/TASVideos.Benchmarks/**'
  schedule:
    - cron: '0 2 * * 0' # Weekly on Sunday at 2 AM UTC

jobs:
  benchmark:
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v4
      with:
        fetch-depth: 0 # Fetch all history for comparison

    - name: Setup .NET
      uses: actions/setup-dotnet@v4
      with:
        dotnet-version: 8.0.x

    - name: Restore dependencies
      run: dotnet restore

    - name: Build benchmarks (Release)
      run: dotnet build tests/TASVideos.Benchmarks/TASVideos.Benchmarks.csproj -c Release --no-restore

    - name: Run benchmarks
      run: |
        cd tests/TASVideos.Benchmarks
        dotnet run -c Release --no-build --exporters json markdown html
      continue-on-error: true

    - name: Upload benchmark results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: benchmark-results-${{ github.sha }}
        path: tests/TASVideos.Benchmarks/BenchmarkDotNet.Artifacts/results/**/*
        retention-days: 90

    - name: Download previous benchmark results
      uses: actions/download-artifact@v4
      if: github.event_name == 'pull_request'
      continue-on-error: true
      with:
        name: benchmark-results-main
        path: baseline

    - name: Compare with baseline
      if: github.event_name == 'pull_request'
      continue-on-error: true
      run: |
        echo "## Benchmark Results" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "Benchmark artifacts have been uploaded for analysis." >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY

        # Find the latest markdown report
        LATEST_REPORT=$(find tests/TASVideos.Benchmarks/BenchmarkDotNet.Artifacts/results -name "*.md" -type f | head -n 1)

        if [ -f "$LATEST_REPORT" ]; then
          echo "### Latest Benchmark Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          cat "$LATEST_REPORT" >> $GITHUB_STEP_SUMMARY
        fi

    - name: Comment PR with results
      if: github.event_name == 'pull_request'
      continue-on-error: true
      uses: actions/github-script@v7
      with:
        script: |
          const fs = require('fs');
          const path = require('path');

          // Find markdown report
          const resultsDir = 'tests/TASVideos.Benchmarks/BenchmarkDotNet.Artifacts/results';

          if (!fs.existsSync(resultsDir)) {
            console.log('No results directory found');
            return;
          }

          const files = fs.readdirSync(resultsDir);
          const mdFile = files.find(f => f.endsWith('.md'));

          if (!mdFile) {
            console.log('No markdown report found');
            return;
          }

          const content = fs.readFileSync(path.join(resultsDir, mdFile), 'utf8');

          const body = `## ðŸ“Š Performance Benchmark Results

          <details>
          <summary>Click to expand benchmark results</summary>

          ${content}

          </details>

          **Note**: These benchmarks run on GitHub Actions runners and may vary between runs.
          Download the artifacts for detailed analysis.
          `;

          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: body
          });

    - name: Store baseline for main branch
      if: github.ref == 'refs/heads/main'
      uses: actions/upload-artifact@v4
      with:
        name: benchmark-results-main
        path: tests/TASVideos.Benchmarks/BenchmarkDotNet.Artifacts/results/**/*
        retention-days: 365 # Keep baseline for 1 year

  benchmark-summary:
    runs-on: ubuntu-latest
    needs: benchmark
    if: always()

    steps:
    - name: Download benchmark results
      uses: actions/download-artifact@v4
      with:
        name: benchmark-results-${{ github.sha }}
        path: results

    - name: Generate summary report
      run: |
        echo "# Benchmark Execution Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "**Run Date**: $(date -u)" >> $GITHUB_STEP_SUMMARY
        echo "**Commit**: ${{ github.sha }}" >> $GITHUB_STEP_SUMMARY
        echo "**Branch**: ${{ github.ref_name }}" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY

        if [ -d "results" ]; then
          echo "## Available Result Files" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "- Markdown reports: $(find results -name "*.md" | wc -l)" >> $GITHUB_STEP_SUMMARY
          echo "- JSON reports: $(find results -name "*.json" | wc -l)" >> $GITHUB_STEP_SUMMARY
          echo "- HTML reports: $(find results -name "*.html" | wc -l)" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "Download the workflow artifacts to view detailed results." >> $GITHUB_STEP_SUMMARY
        else
          echo "âš ï¸ No results found - benchmark run may have failed" >> $GITHUB_STEP_SUMMARY
        fi
